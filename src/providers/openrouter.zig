const std = @import("std");
const http = @import("../http.zig");
const http_async = @import("../http_async.zig");
const base = @import("base.zig");
const Config = @import("../config.zig").Config;
const AsyncEventLoop = @import("../agent/event_loop.zig").AsyncEventLoop;

/// OpenRouter API provider implementation.
/// OpenRouter provides a unified interface to multiple LLM models.
/// Compatible with OpenAI's API format.
/// Response structure from OpenRouter/OpenAI compatible API.
pub const CompletionResponse = struct {
    id: []const u8,
    model: []const u8,
    choices: []const Choice,
};

/// Choice containing the generated message.
pub const Choice = struct {
    message: Message,
};

/// Message in the response with optional content and tool calls.
pub const Message = struct {
    content: ?[]const u8 = null,
    role: []const u8,
    tool_calls: ?[]const ToolCallResponse = null,
};

/// Tool call response from the API.
pub const ToolCallResponse = struct {
    id: []const u8,
    type: []const u8,
    function: FunctionCallResponse,
};

/// Function call details within a tool call.
pub const FunctionCallResponse = struct {
    name: []const u8,
    arguments: []const u8,
};

/// Provider context for OpenRouter API.
/// Holds configuration and HTTP clients.
pub const OpenRouterProvider = struct {
    allocator: std.mem.Allocator,
    client: http.Client,
    async_client: ?http_async.AsyncClient = null,
    api_key: []const u8,
    api_base: []const u8 = "https://openrouter.ai/api/v1",
    event_loop: ?*AsyncEventLoop = null,

    /// Initialize provider with API key.
    pub fn init(allocator: std.mem.Allocator, api_key: []const u8) !OpenRouterProvider {
        return .{
            .allocator = allocator,
            .client = try http.Client.init(allocator),
            .api_key = api_key,
        };
    }

    /// Initialize provider with API key and event loop for async operations.
    pub fn initWithEventLoop(allocator: std.mem.Allocator, api_key: []const u8, event_loop: *AsyncEventLoop) !OpenRouterProvider {
        return .{
            .allocator = allocator,
            .client = try http.Client.init(allocator),
            .async_client = try http_async.AsyncClient.init(allocator),
            .api_key = api_key,
            .event_loop = event_loop,
        };
    }

    /// Clean up provider resources.
    pub fn deinit(self: *OpenRouterProvider) void {
        self.client.deinit();
        if (self.async_client) |*client| client.deinit();
    }

    // Core logic is separated into pure functions below.
    // The struct methods serve as convenient IO wrappers.

    pub fn chat(self: *OpenRouterProvider, messages: []const base.LLMMessage, model: []const u8, tools: ?[]const base.ToolDefinition) !base.LLMResponse {
        const url = try std.fmt.allocPrint(self.allocator, "{s}/chat/completions", .{self.api_base});
        defer self.allocator.free(url);

        const body = try buildChatRequestBody(self.allocator, messages, model, tools, false);
        defer self.allocator.free(body);

        const response_body = try self.execPost(url, body);
        defer self.allocator.free(response_body);

        return parseChatResponse(self.allocator, response_body);
    }

    pub fn chatAsync(self: *OpenRouterProvider, request_id: []const u8, messages: []const base.LLMMessage, model: []const u8, callback: *const fn (result: ChatAsyncResult) void) !void {
        if (self.async_client == null or self.event_loop == null) {
            return error.AsyncNotInitialized;
        }

        const url = try std.fmt.allocPrint(self.allocator, "{s}/chat/completions", .{self.api_base});

        // Use pure function for body construction
        const body = try buildChatRequestBody(self.allocator, messages, model, null, false); // Async chat logic didn't support tools yet in original code? Or did it? Original expected tools: ?[]const base.ToolDefinition was not in arguments for chatAsync signature in provided code. Wait.
        // Original signature: pub fn chatAsync(self: *OpenRouterProvider, request_id: []const u8, messages: []const base.LLMMessage, model: []const u8, callback: *const fn (result: ChatAsyncResult) void) !void
        // It didn't take tools.

        const auth_header = try std.fmt.allocPrint(self.allocator, "Bearer {s}", .{self.api_key});

        const headers = try self.allocator.alloc(std.http.Header, 3);
        headers[0] = .{ .name = "Authorization", .value = auth_header };
        headers[1] = .{ .name = "Content-Type", .value = "application/json" };
        headers[2] = .{ .name = "User-Agent", .value = "satibot/1.0" };

        const wrapper = struct {
            provider: *OpenRouterProvider,
            original_callback: *const fn (result: ChatAsyncResult) void,

            fn httpResultCallback(wr: *const @This(), result: http_async.AsyncClient.AsyncResult) void {
                if (result.success) {
                    const llm_response = parseChatResponse(wr.provider.allocator, result.response.?.body) catch |err| {
                        const error_result = ChatAsyncResult{
                            .request_id = result.request_id,
                            .success = false,
                            .err_msg = std.fmt.allocPrint(wr.provider.allocator, "Failed to parse response: {any}", .{err}) catch unreachable,
                        };
                        wr.original_callback(error_result);
                        return;
                    };

                    const success_result = ChatAsyncResult{
                        .request_id = result.request_id,
                        .success = true,
                        .response = llm_response,
                    };
                    wr.original_callback(success_result);
                } else {
                    const error_result = ChatAsyncResult{
                        .request_id = result.request_id,
                        .success = false,
                        .err_msg = wr.provider.allocator.dupe(u8, result.err_msg.?),
                    };
                    wr.original_callback(error_result);
                }
            }
        };

        const wrapper_instance = wrapper{
            .provider = self,
            .original_callback = callback,
        };

        try self.async_client.?.postAsync(request_id, url, headers, body, &wrapper_instance.httpResultCallback, self.allocator);
    }

    pub fn chatStream(self: *OpenRouterProvider, messages: []const base.LLMMessage, model: []const u8, tools: ?[]const base.ToolDefinition, callback: base.ChunkCallback, cb_ctx: ?*anyopaque) !base.LLMResponse {
        // Stream implementation is complex to separate IO entirely without a generator/iterator
        // But we can reuse request building.
        const url = try std.fmt.allocPrint(self.allocator, "{s}/chat/completions", .{self.api_base});
        defer self.allocator.free(url);

        const body = try buildChatRequestBody(self.allocator, messages, model, tools, true);
        defer self.allocator.free(body);

        const auth_header = try std.fmt.allocPrint(self.allocator, "Bearer {s}", .{self.api_key});
        defer self.allocator.free(auth_header);

        const headers = &[_]std.http.Header{
            .{ .name = "Authorization", .value = auth_header },
            .{ .name = "Content-Type", .value = "application/json" },
            .{ .name = "User-Agent", .value = "satibot/1.0" },
        };

        var req = try self.client.postStream(url, headers, body);
        defer req.deinit();

        var head_buf: [4096]u8 = undefined;
        var response = try req.receiveHead(&head_buf);

        if (response.head.status != .ok) {
            const err_msg = try parseErrorStream(self.allocator, response.head.status, response.reader(&head_buf)); // separated logic
            defer self.allocator.free(err_msg);
            callback(cb_ctx, err_msg);
            return error.ApiRequestFailed;
        }

        // Logic for rate limits and body parsing... I'll keep some IO here but refactor complex parsing logic if possible
        // Actually the stream parsing loop is quite entangled with buffer reading.
        // I will keep it mostly as is for now but use helper where possible.
        // ... (Rate limit logic same as before)
        if (response.head.rate_limit_remaining) |remaining| {
            if (response.head.rate_limit_limit) |limit| {
                const limit_msg = try std.fmt.allocPrint(self.allocator, "ðŸ“Š Rate Limit: {d}/{d}\n\n", .{ remaining, limit });
                defer self.allocator.free(limit_msg);
                callback(cb_ctx, limit_msg);
            }
        }

        return processStreamResponse(self.allocator, &response, callback, cb_ctx);
    }

    pub fn embeddings(self: *OpenRouterProvider, request: base.EmbeddingRequest) !base.EmbeddingResponse {
        const url = try std.fmt.allocPrint(self.allocator, "{s}/embeddings", .{self.api_base});
        defer self.allocator.free(url);

        const body = try std.json.Stringify.valueAlloc(self.allocator, request, .{});
        defer self.allocator.free(body);

        const response_body = try self.execPost(url, body);
        defer self.allocator.free(response_body);

        return parseEmbeddingsResponse(self.allocator, response_body);
    }

    // Private IO helper
    fn execPost(self: *OpenRouterProvider, url: []const u8, body: []const u8) ![]u8 {
        const auth_header = try std.fmt.allocPrint(self.allocator, "Bearer {s}", .{self.api_key});
        defer self.allocator.free(auth_header);

        const headers = &[_]std.http.Header{
            .{ .name = "Authorization", .value = auth_header },
            .{ .name = "Content-Type", .value = "application/json" },
            .{ .name = "User-Agent", .value = "satibot/1.0" },
        };

        const response = try self.client.post(url, headers, body);
        defer self.allocator.free(response.body);

        if (response.status != .ok) {
            const err_msg = try parseErrorBody(self.allocator, response.body);
            defer self.allocator.free(err_msg);
            std.debug.print("[OpenRouter] API request failed with status {d}: {s}\n", .{ @intFromEnum(response.status), err_msg });
            return error.ApiRequestFailed;
        }

        if (response.rate_limit_remaining) |remaining| {
            if (response.rate_limit_limit) |limit| {
                std.debug.print("[OpenRouter] Rate Limit: {d}/{d}\n", .{ remaining, limit });
            }
        }

        return try self.allocator.dupe(u8, response.body);
    }
};

/// Result of an async chat completion
pub const ChatAsyncResult = struct {
    request_id: []const u8,
    success: bool,
    response: ?base.LLMResponse = null,
    err_msg: ?[]const u8 = null,

    pub fn deinit(self: *ChatAsyncResult, allocator: std.mem.Allocator) void {
        if (self.response) |resp| resp.deinit();
        if (self.err_msg) |err| allocator.free(err);
    }
};

// --- Pure Functions (Functional Logic) ---

/// Builds the JSON request body for chat completions.
/// Pure function: depends only on inputs, no side effects.
pub fn buildChatRequestBody(allocator: std.mem.Allocator, messages: []const base.LLMMessage, model: []const u8, tools: ?[]const base.ToolDefinition, stream: bool) ![]u8 {
    var json_buf = std.ArrayListUnmanaged(u8){};
    defer json_buf.deinit(allocator);
    const writer = json_buf.writer(allocator);

    try writer.writeAll("{");
    try writer.print("\"model\": \"{s}\"", .{model});
    if (stream) try writer.writeAll(",\"stream\": true");

    try writer.writeAll(",\"messages\": ");
    const msgs_json = try std.json.Stringify.valueAlloc(allocator, messages, .{});
    defer allocator.free(msgs_json);
    try writer.writeAll(msgs_json);

    if (tools) |t_list| {
        if (t_list.len > 0) {
            try writer.writeAll(",\"tools\": [");
            for (t_list, 0..) |t, i| {
                if (i > 0) try writer.writeAll(",");
                try writer.writeAll("{\"type\": \"function\", \"function\": {");
                try writer.writeAll("\"name\": ");
                try writeEscaped(writer, t.name);
                try writer.writeAll(",\"description\": ");
                try writeEscaped(writer, t.description);
                try writer.writeAll(",\"parameters\": ");
                try writer.writeAll(t.parameters); // Parameters is already valid JSON
                try writer.writeAll("}}");
            }
            try writer.writeAll("]");
        }
    }
    try writer.writeAll("}");
    return json_buf.toOwnedSlice(allocator);
}

/// Helper to write escaped JSON strings.
fn writeEscaped(writer: anytype, text: []const u8) !void {
    try writer.writeAll("\"");
    for (text) |c| {
        switch (c) {
            '"' => try writer.writeAll("\\\""),
            '\\' => try writer.writeAll("\\\\"),
            '\n' => try writer.writeAll("\\n"),
            '\r' => try writer.writeAll("\\r"),
            '\t' => try writer.writeAll("\\t"),
            else => try writer.writeByte(c),
        }
    }
    try writer.writeAll("\"");
}

/// Parses the API response body into a LLMResponse.
/// Pure function.
pub fn parseChatResponse(allocator: std.mem.Allocator, body: []const u8) !base.LLMResponse {
    const parsed = try std.json.parseFromSlice(CompletionResponse, allocator, body, .{ .ignore_unknown_fields = true });
    defer parsed.deinit();

    // std.debug.print("[Model: {s}]\n", .{parsed.value.model}); // Side effect (debug print) ok for now or remove?

    if (parsed.value.choices.len == 0) {
        return error.NoChoicesReturned;
    }

    const msg = parsed.value.choices[0].message;

    var tool_calls: ?[]base.ToolCall = null;
    if (msg.tool_calls) |calls| {
        tool_calls = try allocator.alloc(base.ToolCall, calls.len);
        var allocated: usize = 0;
        errdefer {
            for (0..allocated) |i| {
                allocator.free(tool_calls.?[i].id);
                allocator.free(tool_calls.?[i].function.name);
                allocator.free(tool_calls.?[i].function.arguments);
            }
            allocator.free(tool_calls.?);
        }
        for (calls, 0..) |call, i| {
            tool_calls.?[i] = .{
                .id = try allocator.dupe(u8, call.id),
                .type = "function",
                .function = .{
                    .name = try allocator.dupe(u8, call.function.name),
                    .arguments = try allocator.dupe(u8, call.function.arguments),
                },
            };
            allocated += 1;
        }
    }

    return base.LLMResponse{
        .content = if (msg.content) |c| try allocator.dupe(u8, c) else null,
        .tool_calls = tool_calls,
        .allocator = allocator,
    };
}

/// Parses error response body.
/// Pure function.
pub fn parseErrorBody(allocator: std.mem.Allocator, body: []const u8) ![]u8 {
    const ErrorResponse = struct {
        @"error": struct {
            message: []const u8,
        },
    };
    if (std.json.parseFromSlice(ErrorResponse, allocator, body, .{ .ignore_unknown_fields = true })) |parsed_err| {
        defer parsed_err.deinit();
        return try allocator.dupe(u8, parsed_err.value.@"error".message);
    } else |_| {
        return try allocator.dupe(u8, body);
    }
}

/// Parses embeddings response.
/// Pure function.
pub fn parseEmbeddingsResponse(allocator: std.mem.Allocator, body: []const u8) !base.EmbeddingResponse {
    const EmbeddingsData = struct {
        embedding: []f32,
    };
    const EmbeddingsResponse = struct {
        data: []EmbeddingsData,
    };

    const parsed = try std.json.parseFromSlice(EmbeddingsResponse, allocator, body, .{ .ignore_unknown_fields = true });
    defer parsed.deinit();

    var result = try allocator.alloc([]const f32, parsed.value.data.len);
    var allocated: usize = 0;
    errdefer {
        for (0..allocated) |i| allocator.free(result[i]);
        allocator.free(result);
    }

    for (parsed.value.data, 0..) |item, i| {
        result[i] = try allocator.dupe(f32, item.embedding);
        allocated += 1;
    }

    return base.EmbeddingResponse{
        .embeddings = result,
        .allocator = allocator,
    };
}

// Helper for stream error parsing (involves reader, so not strictly pure string input, but separate logic)
fn parseErrorStream(allocator: std.mem.Allocator, status: std.http.Status, err_reader: anytype) ![]u8 {
    var err_body = std.ArrayListUnmanaged(u8){};
    defer err_body.deinit(allocator);
    var buf: [1024]u8 = undefined;
    var reader = err_reader;
    while (true) {
        const n = try reader.read(&buf);
        if (n == 0) break;
        try err_body.appendSlice(allocator, buf[0..n]);
    }

    const raw_msg = err_body.items;
    const ErrorResponse = struct {
        @"error": struct {
            message: []const u8,
        },
    };

    // std.debug.print("[OpenRouter] Error Body: {s}\n", .{raw_msg}); // Debug log

    if (std.json.parseFromSlice(ErrorResponse, allocator, raw_msg, .{ .ignore_unknown_fields = true })) |parsed| {
        defer parsed.deinit();
        return std.fmt.allocPrint(allocator, "API request failed with status {d}: {s}", .{ @intFromEnum(status), parsed.value.@"error".message });
    } else |_| {
        return std.fmt.allocPrint(allocator, "API request failed with status {d}: {s}", .{ @intFromEnum(status), raw_msg });
    }
}

fn processStreamResponse(allocator: std.mem.Allocator, response: *http.Request.IncomingResponse, callback: base.ChunkCallback, cb_ctx: ?*anyopaque) !base.LLMResponse {
    var full_content = std.ArrayListUnmanaged(u8){};
    errdefer full_content.deinit(allocator);

    var response_body_buf: [8192]u8 = undefined;
    var reader = response.reader(&response_body_buf);

    // Tool calls are delivered in chunks
    var tool_calls_map = std.AutoHashMap(usize, struct {
        id: std.ArrayListUnmanaged(u8),
        name: std.ArrayListUnmanaged(u8),
        arguments: std.ArrayListUnmanaged(u8),
    }).init(allocator);
    defer {
        var it = tool_calls_map.valueIterator();
        while (it.next()) |call| {
            call.id.deinit(allocator);
            call.name.deinit(allocator);
            call.arguments.deinit(allocator);
        }
        tool_calls_map.deinit();
    }

    var buffer = std.ArrayListUnmanaged(u8){};
    defer buffer.deinit(allocator);

    while_read: while (true) {
        var read_buf: [4096]u8 = undefined;
        const bytes_read = reader.readSliceShort(&read_buf) catch |err| {
            if (err == error.EndOfStream) break;
            return err;
        };
        if (bytes_read == 0) break;

        try buffer.appendSlice(allocator, read_buf[0..bytes_read]);

        while (true) {
            const newline_pos = std.mem.indexOfScalar(u8, buffer.items, '\n') orelse break;
            const line = buffer.items[0..newline_pos];
            const trimmed = std.mem.trimLeft(u8, line, " \r\n");

            if (trimmed.len > 0) {
                if (std.mem.startsWith(u8, trimmed, "data: ")) {
                    const data = std.mem.trim(u8, trimmed[6..], " \r\n");
                    if (std.mem.eql(u8, data, "[DONE]")) {
                        break :while_read;
                    } else {
                        const ChunkResponse = struct {
                            choices: []struct {
                                delta: struct {
                                    content: ?[]const u8 = null,
                                    tool_calls: ?[]struct {
                                        index: usize,
                                        id: ?[]const u8 = null,
                                        type: ?[]const u8 = null,
                                        function: ?struct {
                                            name: ?[]const u8 = null,
                                            arguments: ?[]const u8 = null,
                                        } = null,
                                    } = null,
                                },
                            },
                        };

                        if (std.json.parseFromSlice(ChunkResponse, allocator, data, .{ .ignore_unknown_fields = true })) |parsed| {
                            defer parsed.deinit();
                            if (parsed.value.choices.len > 0) {
                                const delta = parsed.value.choices[0].delta;
                                if (delta.content) |content| {
                                    try full_content.appendSlice(allocator, content);
                                    callback(cb_ctx, content);
                                }
                                if (delta.tool_calls) |calls| {
                                    for (calls) |call| {
                                        var entry = try tool_calls_map.getOrPut(call.index);
                                        if (!entry.found_existing) {
                                            entry.value_ptr.* = .{
                                                .id = std.ArrayListUnmanaged(u8){},
                                                .name = std.ArrayListUnmanaged(u8){},
                                                .arguments = std.ArrayListUnmanaged(u8){},
                                            };
                                        }
                                        if (call.id) |id| try entry.value_ptr.id.appendSlice(allocator, id);
                                        if (call.function) |f| {
                                            if (f.name) |n| try entry.value_ptr.name.appendSlice(allocator, n);
                                            if (f.arguments) |args| try entry.value_ptr.arguments.appendSlice(allocator, args);
                                        }
                                    }
                                }
                            }
                        } else |err| {
                            std.debug.print("\n[OpenRouter] Failed to parse chunk JSON: {any} Data: {s}\n", .{ err, data });
                        }
                    }
                }
            }
            try buffer.replaceRange(allocator, 0, newline_pos + 1, &.{});
        }
    }

    var final_tool_calls: ?[]base.ToolCall = null;
    if (tool_calls_map.count() > 0) {
        final_tool_calls = try allocator.alloc(base.ToolCall, tool_calls_map.count());
        var i: usize = 0;
        var it = tool_calls_map.iterator();
        while (it.next()) |entry| {
            final_tool_calls.?[i] = .{
                .id = try entry.value_ptr.id.toOwnedSlice(allocator),
                .type = "function",
                .function = .{
                    .name = try entry.value_ptr.name.toOwnedSlice(allocator),
                    .arguments = try entry.value_ptr.arguments.toOwnedSlice(allocator),
                },
            };
            i += 1;
        }
    }

    return base.LLMResponse{
        .content = if (full_content.items.len > 0) try full_content.toOwnedSlice(allocator) else null,
        .tool_calls = final_tool_calls,
        .allocator = allocator,
    };
}

test "OpenRouter: parse response" {
    const allocator = std.testing.allocator;
    var provider = try OpenRouterProvider.init(allocator, "test-key");
    defer provider.deinit();

    try std.testing.expectEqualStrings("test-key", provider.api_key);
}

test "OpenRouter: struct definitions" {
    // Test FunctionCallResponse
    const func = FunctionCallResponse{
        .name = "test_function",
        .arguments = "{\"key\": \"value\"}",
    };
    try std.testing.expectEqualStrings("test_function", func.name);
    try std.testing.expectEqualStrings("{\"key\": \"value\"}", func.arguments);

    // Test ToolCallResponse
    const tool_call = ToolCallResponse{
        .id = "call_123",
        .type = "function",
        .function = func,
    };
    try std.testing.expectEqualStrings("call_123", tool_call.id);
    try std.testing.expectEqualStrings("function", tool_call.type);
    try std.testing.expectEqualStrings("test_function", tool_call.function.name);

    // ... (Other tests remain valid as structs didn't change name/layout)
}

test "OpenRouter: init and deinit" {
    const allocator = std.testing.allocator;
    var provider = try OpenRouterProvider.init(allocator, "my-api-key-123");
    defer provider.deinit();

    try std.testing.expectEqual(allocator, provider.allocator);
    try std.testing.expectEqualStrings("my-api-key-123", provider.api_key);
}

// Additional tests for pure functions
test "OpenRouter pure: buildChatRequestBody" {
    const allocator = std.testing.allocator;
    const messages = &[_]base.LLMMessage{
        .{ .role = "user", .content = "hello" },
    };
    const body = try buildChatRequestBody(allocator, messages, "gpt-4", null, false);
    defer allocator.free(body);
    // Simple check - verify model is present
    try std.testing.expect(std.mem.indexOf(u8, body, "\"model\": \"gpt-4\"") != null);
    // Verify messages array is present
    try std.testing.expect(std.mem.indexOf(u8, body, "\"messages\"") != null);
}

test "OpenRouter pure: parseChatResponse" {
    const allocator = std.testing.allocator;
    const json =
        \\{
        \\  "id": "chatcmpl-123",
        \\  "model": "gpt-3.5-turbo-0613",
        \\  "choices": [{
        \\    "index": 0,
        \\    "message": {
        \\      "role": "assistant",
        \\      "content": "Hello there!"
        \\    },
        \\    "finish_reason": "stop"
        \\  }]
        \\}
    ;
    var resp = try parseChatResponse(allocator, json);
    defer resp.deinit();

    try std.testing.expectEqualStrings("Hello there!", resp.content.?);
}

/// Get API key for OpenRouter provider
fn getApiKey(ctx: *anyopaque, config: Config) ?[]const u8 {
    _ = ctx;
    return if (config.providers.openrouter) |p| p.apiKey else std.posix.getenv("OPENROUTER_API_KEY");
}

/// Initialize OpenRouter provider
fn initProvider(allocator: std.mem.Allocator, api_key: []const u8) !*anyopaque {
    const provider = try allocator.create(OpenRouterProvider);
    provider.* = try OpenRouterProvider.init(allocator, api_key);
    return provider;
}

/// Deinitialize OpenRouter provider
fn deinitProvider(provider: *anyopaque) void {
    const openrouter_provider: *OpenRouterProvider = @ptrCast(@alignCast(provider));
    const allocator = openrouter_provider.allocator;
    openrouter_provider.deinit();
    allocator.destroy(openrouter_provider);
}

/// Chat stream implementation for OpenRouter provider
fn chatStream(
    provider: *anyopaque,
    messages: []const base.LLMMessage,
    model: []const u8,
    tools: []const base.ToolDefinition,
    chunk_callback: base.ChunkCallback,
    callback_ctx: ?*anyopaque,
) !base.LLMResponse {
    const openrouter_provider: *OpenRouterProvider = @ptrCast(@alignCast(provider));
    return openrouter_provider.chatStream(messages, model, tools, chunk_callback, callback_ctx);
}

/// Get provider name
fn getProviderName() []const u8 {
    return "OpenRouter";
}

/// Create a ProviderInterface for OpenRouter
pub fn createInterface() base.ProviderInterface {
    return base.ProviderInterface{
        .ctx = undefined,
        .getApiKey = getApiKey,
        .initProvider = initProvider,
        .deinitProvider = deinitProvider,
        .chatStream = chatStream,
        .getProviderName = getProviderName,
    };
}
